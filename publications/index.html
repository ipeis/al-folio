<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ignacio Peis </title> <meta name="author" content="Ignacio Peis"/> <meta name="description" content="sorted list of publications"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icon.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ipeis.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://ipeis.github.io/"><span class="font-weight-bold">Ignacio</span> Peis</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">experience</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">sorted list of publications</p> </header> <article> <div class="publications"> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PhD Thesis</abbr></div> <div id="peis2023advanced" class="col-sm-8"> <div class="title">Advanced Inference and Representation Learning Methods in Variational Autoencoders</div> <div class="author">Ignacio Peis </div> <div class="periodical"> <em>PhD Thesis Dissertation</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://ipeis.github.io/assets/pdf/PhD_Thesis_Ignacio_Peis_Defense.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://ipeis.github.io/assets/pdf/PhD_Presentation_Ignacio_Peis.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p> Deep Generative Models have gained significant popularity in the Machine Learning research community since the early 2010s. These models allow to generate realistic data by leveraging the power of Deep Neural Networks. The field experienced a significant breakthrough when Variational Autoencoders (VAEs) were introduced. VAEs revolutionized Deep Generative Modeling by providing a scalable and flexible framework that enables the generation of complex data distributions and the learning of potentially interpretable latent representations. They have proven to be a powerful tool in numerous applications, from image, sound and video generation to natural language processing or drug discovery, among others. At their core, VAEs encode natural information into a reduced latent space and decode the learned latent space into new synthetic data. Advanced versions of VAEs have been developed to handle challenges such as handling heterogeneous incomplete data, encoding into hierarchical latent spaces for representing abstract and richer concepts, or modeling sequential data, among others. These advances have expanded the capabilities of VAEs and made them a valuable tool in a wide range of fields. Despite the significant progress made in VAE research, there is still ample room for improvement in their current state-of-the-art. One of the major challenges is improving their approximate inference. VAEs typically assume Gaussian approximations of the posterior distribution of the latent variables in order to make the training objective tractable. The parameters of this approximation are provided by encoder networks. However, this approximation leads to a lower bounded objective, which can degrade the performance of any task that requires samples from the approximate posterior, due to the implicit bias. The second major challenge addressed in this thesis is related to achieving meaningful latent representations, or more broadly, how the latent space disentangles generative factors of variation. Ideally, the latent space would modulate meaningful properties separately within each dimension. However, Maximum Likelihood optimizations require the marginalization of latent variables, leading to non-unique solutions that may or may not achieve this desired disentanglement. Additionally, properties learned at the observation level in VAEs assume that every observation is generated independently, which may not be the case in some scenarios. To address these limitations, more robust VAEs have been developed to learn disentangled properties at the supervised group (also referred to as global) level. These models are capable of generating groups of data with shared properties. The work presented in this doctoral thesis focuses on the development of novel methods for improving the state-of-the-art in VAEs. Specifically, three fundamental challenges are addressed: achieving meaningful global latent representations, obtaining highly-flexible priors for learning more expressive models, and improving current approximate inference methods. As a first main contribution, an innovative technique named UG-VAE from Unsupervised-Global VAE, aims to enhance the ability of VAEs in capturing factors of variations at data (local) and group (global) level. By carefully desigining the encoder and the decoder, and throughout conductive experiments, it is demonstrated that UG-VAE is effective in capturing unsupervised global factors from images. Second, a non-trivial combination of highly-expressive Hierarchical VAEs with robust Markov Chain Monte Carlo inference (specifically Hamiltonian Monte Carlo), for which important issues are successfully resolved, is presented. The resulting model, referred to as the Hierarchical Hamiltonian VAE model for Mixed-type incomplete data (HH-VAEM), addresses the challenges associated with imputing and acquiring heterogeneous missing data. Throughout extensive experiments, it is demonstrated that HH-VAEM outperforms existing one-layered and Gaussian baselines in the tasks of missing data imputation and supervised learning with missing features, thanks to its improved inference and expressivity. Furthermore, another relevant contribution is presented, namely a sampling-based approach for efficiently computing the information gain when missing features are to be acquired with HH-VAEM. This approach leverages the advantages of HH-VAEM and is demonstrated to be effective in the same tasks. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="koyuncu2023variational" class="col-sm-8"> <div class="title">Variational Mixture of HyperGenerators for Learning Distributions Over Functions</div> <div class="author"> <a href="https://www.batukoyuncu.com/" target="_blank" rel="noopener noreferrer">Batuhan Koyuncu</a>, <a href="https://is.mpg.de/person/psanchez" target="_blank" rel="noopener noreferrer">Pablo Sanchez-Martin</a>, Ignacio Peis, <a href="https://www.tsc.uc3m.es/~olmos/" target="_blank" rel="noopener noreferrer">Pablo M. Olmos</a>, and <a href="https://ivaleram.github.io/" target="_blank" rel="noopener noreferrer">Isabel Valera</a> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://proceedings.mlr.press/v202/koyuncu23a/koyuncu23a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/bkoyuncu/vamoh" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://icml.cc/media/PosterPDFs/ICML%202023/23684.png?t=1689852186.8291144" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> <div class="abstract hidden"> <p> Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally intensive when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VAMoH. VAMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VAMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VAMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VAMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better than previous approaches, while being less computationally demanding. </p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="peis2022missing" class="col-sm-8"> <div class="title">Missing Data Imputation and Acquisition with Deep Hierarchical Models and Hamiltonian Monte Carlo</div> <div class="author">Ignacio Peis, <a href="https://chao-ma.org/" target="_blank" rel="noopener noreferrer">Chao Ma</a>, and <a href="https://jmhl.org/" target="_blank" rel="noopener noreferrer">José Miguel Hernández-Lobato</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems 35</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://papers.nips.cc/paper_files/paper/2022/file/e8dbeb1c947a30576c699e7f5c73d3e3-Supplemental-Conference.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/ipeis/HH-VAEM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://ipeis.github.io/assets/pdf/NeurIPS_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://ipeis.github.io/assets/pdf/NeurIPS_presentation.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Variational Autoencoders (VAEs) have recently been highly successful at imputing and acquiring heterogeneous missing data. However, within this specific application domain, existing VAE methods are restricted by using only one layer of latent variables and strictly Gaussian posterior approximations. To address these limitations, we present HH-VAEM, a Hierarchical VAE model for mixed-type incomplete data that uses Hamiltonian Monte Carlo with automatic hyper-parameter tuning for improved approximate inference. Our experiments show that HH-VAEM outperforms existing baselines in the tasks of missing data imputation and supervised learning with missing features. Finally, we also present a sampling-based approach for efficiently computing the information gain when missing features are to be acquired with HH-VAEM. Our experiments show that this sampling-based approach is superior to alternatives based on Gaussian approximations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PR</abbr></div> <div id="PEIS2023109130" class="col-sm-8"> <div class="title">Unsupervised learning of global factors in deep generative models</div> <div class="author">Ignacio Peis, <a href="https://www.tsc.uc3m.es/~olmos/" target="_blank" rel="noopener noreferrer">Pablo M. Olmos</a>, and <a href="https://www.tsc.uc3m.es/~antonio/antonio_artes/Home.html" target="_blank" rel="noopener noreferrer">Antonio Artés-Rodríguez</a> </div> <div class="periodical"> <em>Pattern Recognition</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.sciencedirect.com/science/article/pii/S0031320322006100" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a> <a href="https://www.sciencedirect.com/science/article/pii/S0031320322006100/pdfft?isDTMRedir=true&amp;download=true" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/ipeis/UG-VAE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We present a novel deep generative model based on non i.i.d. variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. In contrast to the recent semi-supervised alternatives for global modeling in deep generative models, our approach combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead us to obtain three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in β-VAE and its generalizations). Second, we show that the model performs domain alignment to find correlations and interpolate between different databases. Finally, we study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures, such as face images with shared attributes or defined sequences of digits images.</p> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Nature SC</abbr></div> <div id="peis2020actigraphic" class="col-sm-8"> <div class="title">Actigraphic recording of motor activity in depressed inpatients: a novel computational approach to prediction of clinical course and hospital discharge</div> <div class="author">Ignacio Peis, Javier-David López-Morı́ñigo, M Mercedes Pérez-Rodrı́guez, Maria-Luisa Barrigón, Marta Ruiz-Gómez, <a href="https://www.tsc.uc3m.es/~antonio/antonio_artes/Home.html" target="_blank" rel="noopener noreferrer">Antonio Artés-Rodríguez</a>, and Enrique Baca-Garcı́a </div> <div class="periodical"> <em>Scientific reports</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.nature.com/articles/s41598-020-74425-x" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a> </div> <div class="abstract hidden"> <p>Depressed patients present with motor activity abnormalities, which can be easily recorded using actigraphy. The extent to which actigraphically recorded motor activity may predict inpatient clinical course and hospital discharge remains unknown. Participants were recruited from the acute psychiatric inpatient ward at Hospital Rey Juan Carlos (Madrid, Spain). They wore miniature wrist wireless inertial sensors (actigraphs) throughout the admission. We modeled activity levels against the normalized length of admission—‘Progress Towards Discharge’ (PTD)—using a Hierarchical Generalized Linear Regression Model. The estimated date of hospital discharge based on early measures of motor activity and the actual hospital discharge date were compared by a Hierarchical Gaussian Process model. Twenty-three depressed patients (14 females, age: 50.17 ± 12.72 years) were recruited. Activity levels increased during the admission (mean slope of the linear function: 0.12 ± 0.13). For n = 18 inpatients (78.26%) hospitalised for at least 7 days, the mean error of Prediction of Hospital Discharge Date at day 7 was 0.231 ± 22.98 days (95% CI 14.222–14.684). These n = 18 patients were predicted to need, on average, 7 more days in hospital (for a total length of stay of 14 days) (PTD = 0.53). Motor activity increased during the admission in this sample of depressed patients and early patterns of actigraphically recorded activity allowed for accurate prediction of hospital discharge date.</p> </div> </div> </div> </li></ol> <h2 class="year">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JBHI</abbr></div> <div id="peis2019deep" class="col-sm-8"> <div class="title">Deep sequential models for suicidal ideation from multiple source data</div> <div class="author">Ignacio Peis, <a href="https://www.tsc.uc3m.es/~olmos/" target="_blank" rel="noopener noreferrer">Pablo M. Olmos</a>, Constanza Vera-Varela, Marı́a Luisa Barrigón, Philippe Courtet, Enrique Baca-Garcia, and <a href="https://www.tsc.uc3m.es/~antonio/antonio_artes/Home.html" target="_blank" rel="noopener noreferrer">Antonio Artés-Rodríguez</a> </div> <div class="periodical"> <em>IEEE Journal of Biomedical and Health Informatics</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/1911.03522" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://ieeexplore.ieee.org/abstract/document/8723181" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a> </div> <div class="abstract hidden"> <p>This paper presents a novel method for predicting suicidal ideation from electronic health records (EHR) and ecological momentary assessment (EMA) data using deep sequential models. Both EHR longitudinal data and EMA question forms are defined by asynchronous, variable length, randomly sampled data sequences. In our method, we model each of them with a recurrent neural network, and both sequences are aligned by concatenating the hidden state of each of them using temporal marks. Furthermore, we incorporate attention schemes to improve performance in long sequences and time-independent pre-trained schemes to cope with very short sequences. Using a database of 1023 patients, our experimental results show that the addition of EMA records boosts the system recall to predict the suicidal ideation diagnosis from 48.13% obtained exclusively from EHR-based state-of-the-art methods to 67.78%. Additionally, our method provides interpretability through the t-distributed stochastic neighbor embedding (t-SNE) representation of the latent space. Furthermore, the most relevant input features are identified and interpreted medically.</p> </div> </div> </div> </li></ol> <h2 class="year">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Frontiers</abbr></div> <div id="castillo2017heavy" class="col-sm-8"> <div class="title">A heavy tailed expectation maximization hidden markov random field model with applications to segmentation of MRI</div> <div class="author">Diego Castillo-Barnes, Ignacio Peis, Francisco J Martı́nez-Murcia, Fermı́n Segovia, Ignacio A. Illán, Juan M. Górriz, Javier Ramı́rez, and Diego Salas-Gonzalez </div> <div class="periodical"> <em>Frontiers in Neuroinformatics</em>, 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.frontiersin.org/articles/10.3389/fninf.2017.00066/full" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a> </div> <div class="abstract hidden"> <p>A wide range of segmentation approaches assumes that intensity histograms extracted from magnetic resonance images (MRI) have a distribution for each brain tissue that can be modeled by a Gaussian distribution or a mixture of them. Nevertheless, intensity histograms of White Matter and Gray Matter are not symmetric and they exhibit heavy tails. In this work, we present a hidden Markov random field model with expectation maximization (EM-HMRF) modeling the components using the α-stable distribution. The proposed model is a generalization of the widely used EM-HMRF algorithm with Gaussian distributions. We test the α-stable EM-HMRF model in synthetic data and brain MRI data. The proposed methodology presents two main advantages: Firstly, it is more robust to outliers. Secondly, we obtain similar results than using Gaussian when the Gaussian assumption holds. This approach is able to model the spatial dependence between neighboring voxels in tomographic brain MRI.</p> </div> </div> </div> </li></ol> <h2 class="year">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NSS/MIC</abbr></div> <div id="peis2016mri" class="col-sm-8"> <div class="title">MRI brain segmentation using hidden Markov random fields with alpha-stable distributions</div> <div class="author">Ignacio Peis, Francisco J. Martı́nez-Murcia, Fermín Segovia, Juan M. Górriz, Javier Ramı́rez, Elmar W. Lang, and Diego Salas-Gonzalez </div> <div class="periodical"> <em>In 2016 IEEE Nuclear Science Symposium, Medical Imaging Conference and Room-Temperature Semiconductor Detector Workshop (NSS/MIC/RTSD)</em>, 2016 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://ieeexplore.ieee.org/abstract/document/8069422" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a> </div> <div class="abstract hidden"> <p>A MRI brain image segmentation method using a hidden Markov random fields with heavy-tailed alpha-stable distributions is presented. Each brain tissue is modelled using an alpha-stable distribution. Then, a HMRF is used to include spatial information in the classification model. The Gaussian distribution has been widely used for the modelization of the cerebrospinal fluid, white matter and gray matter. Nevertheless, the alpha-stable distribution has been recently proposed as a more accurate alternative for this task. The alpha-stable distribution is more impulsive and is also able to model the asymmetry and heavy-tails of the histogram of the brain tissues. We have tested the proposed methodology in 18 MR images from the Internet Brain Segmentation Repository. The proposed methodology outperforms the segmentation results obtained when a Gaussian model for the histogram of the brain tissues is considered. Furthermore, as the Normal distribution is a particulaar case of alpha-stable distribution. Therefore, the proposed approach is also a generalization of the hidden Markov random field segmentation method with Gaussian distributions.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ignacio Peis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>